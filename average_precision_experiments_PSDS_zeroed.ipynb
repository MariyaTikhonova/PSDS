{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashk\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from scipy.sparse.linalg.eigen import arpack\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.sparse import csc_matrix, lil_matrix, coo_matrix, csr_matrix\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import operator\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mapak(frame, score_col = 'cosine', word_col = 'word1', rel_col = 'SimLex'):\n",
    "    count = 0\n",
    "    mapak = 0.0\n",
    "    for word, df in frame.groupby(word_col):\n",
    "\n",
    "        median_ = df[rel_col].median()\n",
    "\n",
    "        K = (1 * (df[rel_col] >= median_)).sum()\n",
    "        if K == 0:\n",
    "            print(df)\n",
    "        df_sorted = df.sort_values(by = score_col, ascending = False)\n",
    "        apak = 0.0\n",
    "        for k_ in range(K):\n",
    "            k = k_+1\n",
    "            pak = 1.0 * (df_sorted.iloc[:k][rel_col] >= median_).sum()/float(k)\n",
    "            apak += pak\n",
    "        apak /= float(K)\n",
    "\n",
    "        mapak += apak\n",
    "        count += 1\n",
    "    mapak /= float(count)\n",
    "    return mapak\n",
    "\n",
    "def calc_cosines(frame, model):\n",
    "    cosines = []\n",
    "    for ind, row in frame.iterrows():\n",
    "        cosines.append(cosine_similarity([model.wv[row.word1], model.wv[row.word2]])[0][1])\n",
    "    return cosines\n",
    "\n",
    "def calc_V(X, k = 100):\n",
    "    eig_vals, eig_vecs = eigsh(X, k, which = 'LA')\n",
    "    V = np.zeros((X.shape[0], k))\n",
    "    for i in range(k):\n",
    "        V[:, i] = np.sqrt(eig_vals[-1 - i]) * eig_vecs[:,-1 - i]\n",
    "    return V\n",
    "\n",
    "def calc_V(X, k = 100, which_ = 'LA'):\n",
    "    eig_vals, eig_vecs = eigsh(X, k, which = which_)\n",
    "    V = np.zeros((X.shape[0], k))\n",
    "    if which_ == 'LM':\n",
    "        order = np.argsort(np.abs(eig_vals))[::-1]\n",
    "        for i in range(k):\n",
    "            ind = order[i]\n",
    "            V[:, i] = np.sign(eig_vals[ind]) * np.sqrt(np.abs(eig_vals[ind])) * eig_vecs[:,ind]\n",
    "    elif which_ == 'LA':\n",
    "        for i in range(k):\n",
    "            V[:, i] = np.sqrt(eig_vals[-1 - i]) * eig_vecs[:,-1 - i]\n",
    "    return V\n",
    "\n",
    "def calc_cosines_V(frame, V, word_to_num):\n",
    "    cosines = []\n",
    "    for ind, row in frame.iterrows():\n",
    "\n",
    "        cosines.append(cosine_similarity([V[word_to_num[row.word1], :], V[word_to_num[row.word2], :]])[0][1])\n",
    "    return cosines\n",
    "\n",
    "def test_models(k, X, word_to_num, frame, col_name, W2V_ = True, which_ = 'LA'):\n",
    "    print('Vocab size: ', X.shape[0], ' factorization type: ', which_)\n",
    "    size_ = int(k/2)\n",
    "    if X.shape[0] == 10000:\n",
    "        min_count_ = 175\n",
    "    elif X.shape[0] == 100000:\n",
    "        min_count_ = 5\n",
    "    elif X.shape[0] == 50000:\n",
    "        min_count_ = 15\n",
    "    else:\n",
    "        min_count_ = 1\n",
    "        \n",
    "    V = calc_V(X, k, which_)\n",
    "    frame['V'+ col_name] = calc_cosines_V(frame, V, word_to_num)\n",
    "    if W2V_:\n",
    "        model = gensim.models.Word2Vec(split_lines, size=size_, window=2, min_count=min_count_)\n",
    "        frame['w2v_'+ col_name] = calc_cosines(frame, model)\n",
    "\n",
    "        print('comp =', k,'min_c =',min_count_,'\\t',round(calc_mapak(frame, 'w2v_'+ col_name),4), '\\t\\t', round(calc_mapak(frame, 'V'+ col_name),4))\n",
    "    else:\n",
    "        print('comp =', k,'\\t\\t', round(calc_mapak(frame, 'V'+ col_name),4))\n",
    "\n",
    "def calc_X_short(X, word_counts, word_list, vocab_size = 100000, stop_words_num = 29):\n",
    "\n",
    "    stop_inds = [x[0] for x in word_counts.most_common(stop_words_num)]\n",
    "    #print(stop_inds)\n",
    "    stop_words = [word_list[x] for x in stop_inds]\n",
    "\n",
    "    vocab_inds = sorted([x[0] for x in word_counts.most_common(vocab_size + stop_words_num) if not x[0] in stop_inds])\n",
    "    vocab = [word_list[x] for x in vocab_inds]\n",
    "    word_freqs_short = [word_counts[x] for x in vocab_inds]\n",
    "    word_to_num_short = {}\n",
    "    word_list_short = {}\n",
    "    for i, word in enumerate(vocab):\n",
    "        word_to_num_short[word] = i\n",
    "        word_list_short[i] = word\n",
    "\n",
    "    X_short = X[vocab_inds, :]\n",
    "    X_short = X_short[:, vocab_inds]\n",
    "    return X_short, word_list_short, word_to_num_short, word_freqs_short\n",
    "\n",
    "def calc_norm_X(X, word_freqs):\n",
    "    inv_word_freqs = [1.0/float(x) for x in word_freqs]\n",
    "    N = coo_matrix((np.sqrt(inv_word_freqs), (range(len(inv_word_freqs)), range(len(inv_word_freqs)))), shape=(len(inv_word_freqs), len(inv_word_freqs)))\n",
    "    N_csr = N.tocsr()\n",
    "    X_norm = N_csr.dot(X.dot(N_csr))\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [line.rstrip('\\n') for line in open('parsed_simple_wiki.txt', encoding = 'utf-8')]\n",
    "split_lines = [line.split(' ') for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228 645\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>SimLex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new</td>\n",
       "      <td>old</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>8.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>difficult</td>\n",
       "      <td>hard</td>\n",
       "      <td>8.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>easy</td>\n",
       "      <td>hard</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word1      word2 SimLex\n",
       "1        new        old   1.58\n",
       "4       hard  difficult   8.77\n",
       "5  difficult       hard   8.77\n",
       "8       hard       easy   0.95\n",
       "9       easy       hard   0.95"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex_doubled = pickle.load(open('simlex_doubled_cleaned.pkl','rb'))\n",
    "vocab = pickle.load(open('vocab_10000','rb'))\n",
    "word_counts = Counter(simlex_doubled['word1'])\n",
    "word_set3 = set()\n",
    "for word in simlex_doubled.word1.unique():\n",
    "    if word_counts[word] > 2:\n",
    "        word_set3.add(word)\n",
    "\n",
    "simlex_fr3 = simlex_doubled[simlex_doubled.word1.isin(word_set3)]\n",
    "simlex_fr3 = simlex_fr3[simlex_fr3.word1.isin(vocab)]\n",
    "simlex_fr3 = simlex_fr3[simlex_fr3.word2.isin(vocab)]\n",
    "print(len(word_set3), simlex_fr3.shape[0])\n",
    "simlex_fr3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 468473/468473 [00:28<00:00, 16179.96it/s]\n"
     ]
    }
   ],
   "source": [
    "X = pickle.load(open('X_maxtrix5_end.pkl','rb'))\n",
    "for i in tqdm.tqdm(range(X.shape[0])):\n",
    "    X[i, i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = pickle.load( open('word_set_small_wiki.pkl', 'rb'))\n",
    "s0 = pickle.load(open('s0_end.pkl','rb'))\n",
    "#468473, 468473\n",
    "X.shape[0], len(word_set)\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "n = 0\n",
    "for i, word in enumerate(word_set):\n",
    "    word_to_num[word] = i\n",
    "    num_to_word[i] = word\n",
    "    n += 1\n",
    "word_list = list(word_set)\n",
    "word_counts = Counter(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 100000), (100000, 100000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short, word_list_short, word_to_num_short, word_freqs_short = calc_X_short(X, \n",
    "                                        word_counts, word_list, 100000, 29)\n",
    "X_norm = calc_norm_X(X_short, word_freqs_short)\n",
    "X_short.shape, X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  100000  factorization type:  LA\n",
      "LA path\n",
      "comp = 100 min_c = 5 \t 0.6231 \t\t 0.6788\n",
      "Vocab size:  100000  factorization type:  LM\n",
      "LM path\n",
      "comp = 100 \t\t 0.6496\n",
      "Vocab size:  100000  factorization type:  LA\n",
      "LA path\n",
      "comp = 100 \t\t 0.5977\n",
      "Vocab size:  100000  factorization type:  LM\n",
      "LM path\n",
      "comp = 100 \t\t 0.5977\n"
     ]
    }
   ],
   "source": [
    "#На полном SimLex\n",
    "test_models(100, X_short, word_to_num_short, simlex_fr3, '_short100')\n",
    "test_models(100, X_short, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n",
    "test_models(100, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False)\n",
    "test_models(100, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  100000  factorization type:  LA\n",
      "comp = 200 min_c = 5 \t 0.6451 \t\t 0.6891\n",
      "Vocab size:  100000  factorization type:  LM\n",
      "comp = 200 \t\t 0.6707\n",
      "Vocab size:  100000  factorization type:  LA\n",
      "comp = 200 \t\t 0.6197\n",
      "Vocab size:  100000  factorization type:  LM\n",
      "comp = 200 \t\t 0.6197\n"
     ]
    }
   ],
   "source": [
    "#На полном SimLex\n",
    "test_models(200, X_short, word_to_num_short, simlex_fr3, '_short200')\n",
    "test_models(200, X_short, word_to_num_short, simlex_fr3, '_short200', W2V_ = False, which_ = 'LM')\n",
    "test_models(200, X_norm, word_to_num_short, simlex_fr3, '_short200', W2V_ = False)\n",
    "test_models(200, X_norm, word_to_num_short, simlex_fr3, '_short200', W2V_ = False, which_ = 'LM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_short, X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10000), (10000, 10000))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short, word_list_short, word_to_num_short, word_freqs_short = calc_X_short(X, \n",
    "                                        word_counts, word_list, 10000, 29)\n",
    "X_norm = calc_norm_X(X_short, word_freqs_short)\n",
    "X_short.shape, X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10000  factorization type:  LA\n",
      "comp = 100 min_c = 175 \t 0.6381 \t\t 0.6793\n",
      "Vocab size:  10000  factorization type:  LM\n",
      "comp = 100 \t\t 0.6496\n",
      "Vocab size:  10000  factorization type:  LA\n",
      "comp = 100 \t\t 0.6128\n",
      "Vocab size:  10000  factorization type:  LM\n",
      "comp = 100 \t\t 0.6109\n"
     ]
    }
   ],
   "source": [
    "#На полном SimLex\n",
    "test_models(100, X_short, word_to_num_short, simlex_fr3, '_short100')\n",
    "test_models(100, X_short, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n",
    "test_models(100, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False)\n",
    "test_models(100, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10000  factorization type:  LA\n",
      "comp = 200 min_c = 175 \t 0.6553 \t\t 0.6891\n",
      "Vocab size:  10000  factorization type:  LM\n",
      "comp = 200 \t\t 0.677\n",
      "Vocab size:  10000  factorization type:  LA\n",
      "comp = 200 \t\t 0.6452\n",
      "Vocab size:  10000  factorization type:  LM\n",
      "comp = 200 \t\t 0.6211\n"
     ]
    }
   ],
   "source": [
    "#На полном SimLex\n",
    "test_models(200, X_short, word_to_num_short, simlex_fr3, '_short200')\n",
    "test_models(200, X_short, word_to_num_short, simlex_fr3, '_short200', W2V_ = False, which_ = 'LM')\n",
    "test_models(200, X_norm, word_to_num_short, simlex_fr3, '_short200', W2V_ = False)\n",
    "test_models(200, X_norm, word_to_num_short, simlex_fr3, '_short200', W2V_ = False, which_ = 'LM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 50000), (50000, 50000))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_short, word_list_short, word_to_num_short, word_freqs_short = calc_X_short(X, \n",
    "                                        word_counts, word_list, 50000, 29)\n",
    "X_norm = calc_norm_X(X_short, word_freqs_short)\n",
    "X_short.shape, X_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  50000  factorization type:  LA\n",
      "comp = 100 min_c = 15 \t 0.6285 \t\t 0.6788\n",
      "Vocab size:  50000  factorization type:  LM\n",
      "comp = 100 \t\t 0.6496\n",
      "Vocab size:  50000  factorization type:  LA\n",
      "comp = 100 \t\t 0.6108\n",
      "Vocab size:  50000  factorization type:  LM\n",
      "comp = 100 \t\t 0.6108\n"
     ]
    }
   ],
   "source": [
    "#На полном SimLex\n",
    "test_models(100, X_short, word_to_num_short, simlex_fr3, '_short100')\n",
    "test_models(100, X_short, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n",
    "test_models(100, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False)\n",
    "test_models(100, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  50000  factorization type:  LA\n",
      "comp = 200 min_c = 15 \t 0.6512 \t\t 0.6891\n",
      "Vocab size:  50000  factorization type:  LM\n",
      "comp = 200 \t\t 0.6707\n",
      "Vocab size:  50000  factorization type:  LA\n",
      "comp = 200 \t\t 0.6258\n",
      "Vocab size:  50000  factorization type:  LM\n",
      "comp = 200 \t\t 0.613\n"
     ]
    }
   ],
   "source": [
    "#На полном SimLex\n",
    "test_models(200, X_short, word_to_num_short, simlex_fr3, '_short100')\n",
    "test_models(200, X_short, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n",
    "test_models(200, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False)\n",
    "test_models(200, X_norm, word_to_num_short, simlex_fr3, '_short100', W2V_ = False, which_ = 'LM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
