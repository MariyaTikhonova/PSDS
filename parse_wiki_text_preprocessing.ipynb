{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy.sparse.linalg import eigsh\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.sparse import csc_matrix, lil_matrix, coo_matrix, csr_matrix\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "import operator\n",
    "import pickle\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление симметричной матрицы слов $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для анализа я взяла dump SimpleWiki и распарсила его с помощью парсера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего я работаю с 84153 статьями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84153\n"
     ]
    }
   ],
   "source": [
    "#lines = ['i love tea', 'my friend', 'i am bee']\n",
    "lines = [line.rstrip('\\n') for line in open('parsed_simple_wiki.txt', encoding = 'utf-8')]\n",
    "#lines = pickle.load(open('parsed_simple_wiki_cleaned_en.pkl','rb'))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст после обработки выглядит вот так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "april is the fourth month of the year and comes between march and may it is one of four months to \n"
     ]
    }
   ],
   "source": [
    "print(lines[0][:98])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь уникальных слов имеет смысл предподсчитать один раз и сохранить, так как это достаточно долгий процесс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 84153/84153 [45:18<00:00, 30.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_set = set()\n",
    "N = 0\n",
    "for line in tqdm(lines):\n",
    "    words = line.split(' ')\n",
    "    N += len(words)\n",
    "    word_set = word_set.union(set(words))\n",
    "word_set = list(word_set)\n",
    "print(len(word_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(word_set, open('word_set_small_wiki_en.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#word_set = pickle.load( open('word_set_small_wiki.pkl', 'rb'))\n",
    "\n",
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "n = 0\n",
    "for i, word in enumerate(word_set):\n",
    "    word_to_num[word] = i\n",
    "    num_to_word[i] = word\n",
    "    n += 1\n",
    "word_list = list(word_set)\n",
    "del word_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {}\n",
    "num_to_word = {}\n",
    "n = 0\n",
    "for i, word in enumerate(word_set):\n",
    "    word_to_num[word] = i\n",
    "    num_to_word[i] = word\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент я работаю с пятиграммами (слово и два соседних). Я учитываю конец статьи, но пока не учитываю границы предложения. Т. к. во всех найденных мной скриптах знаки препинания пропадают при парсинге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.09\n",
      "3.38\n",
      "7.02\n",
      "7.36\n",
      "7.41\n",
      "8.84\n",
      "8.7\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start_ = time()\n",
    "ss = [[word_to_num[w] for w in line.split(' ')] for line in lines]\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()\n",
    "s0 = [s for s in chain.from_iterable(ss)]\n",
    "N_ = len(s0)\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()\n",
    "s1 = [s for s in chain.from_iterable([[s_cur[0]] + s_cur[:-1]  for s_cur in chain(ss)])]\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()\n",
    "s2 = [s for s in chain.from_iterable([[s_cur[0]] + [s_cur[0]] + s_cur[:-2]  for s_cur in chain(ss)])]\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()\n",
    "s1_ = [s for s in chain.from_iterable([[s_cur[0]] + [s_cur[0]] + s_cur[:-2] for s_cur in chain(ss)])]\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()\n",
    "s1_ = [s for s in chain.from_iterable([s_cur[1:] + [s_cur[-1]]  for s_cur in chain(ss)])]\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()\n",
    "s2_ = [s for s in chain.from_iterable([s_cur[2:] + [s_cur[-1]] + [s_cur[-1]]  for s_cur in chain(ss)])]\n",
    "print(round(time()-start_,2))\n",
    "start_ = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468473, 22592884)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_list = list(word_set)\n",
    "N_ = len(s0)\n",
    "n, N_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468473, 22592884)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_list = list(word_set)\n",
    "N_ = len(s0)\n",
    "n, N_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(s0, open('s0_end.pkl','wb'))\n",
    "#pickle.dump(s1, open('s1_en.pkl','wb'))\n",
    "#pickle.dump(s1_, open('s1__en.pkl','wb'))\n",
    "#pickle.dump(s2, open('s2_en.pkl','wb'))\n",
    "#pickle.dump(s2_, open('s2__en.pkl','wb'))\n",
    "#s0 = pickle.load(open('s0_end.pkl','rb'))\n",
    "#s1 = pickle.load(open('s1_end.pkl','rb'))\n",
    "#s1_ = pickle.load(open('s1__end.pkl','rb'))\n",
    "#s2 = pickle.load(open('s2_end.pkl','rb'))\n",
    "#s2_ = pickle.load(open('s2__end.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Подготавливаем индексы для матрицы (окрестность - правый и левый сосед)\n",
    "s_len = list(range(len(s0)))\n",
    "row = np.array(s1 + s1_+ s2 + s2_)\n",
    "col = np.array(s_len + s_len + s_len + s_len)\n",
    "data = np.ones(len(row))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Задаем матрицу в coo формате.\n",
    "A = coo_matrix((data, (row, col)), shape=(n, N_))\n",
    "del data, row, col\n",
    "#Переводим ее в csr формат, чтобы выполнять с ней необходимые операции.\n",
    "A_csr = A.tocsr()\n",
    "A_csr[A.nonzero()] = 1\n",
    "#Вычисляем матрицу X\n",
    "X = A_csr.dot(A_csr.transpose())\n",
    "#print(A_csr.todense())\n",
    "del A, A_csr, s1, s1_, s2, s2_, s_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(s0, open('s0_en.pkl','wb'))\n",
    "pickle.dump(X, open('X_maxtrix5_end.pkl','wb'))\n",
    "#s0 = pickle.load(open('s0.pkl','rb'))\n",
    "#X = pickle.load(open('X_maxtrix5_end.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468473, 468473)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'in', 'and', 'to', 'is', 'was', 'for', 'it', 'on', 'he', 'as', 'by', 'with', 'that', 'are', 'from', 'at', 'an', 'his', 'or', 'they', 'other', 'this', 'be', 'also', 'were', 'not', 'has']\n",
      "100000\n",
      "[6, 8, 17]\n",
      "(100000, 100000)\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(s0)\n",
    "stop_inds = [x[0] for x in word_counts.most_common(29)]\n",
    "stop_words = [word_list[x] for x in stop_inds]\n",
    "\n",
    "#['the', 'of', 'in', 'and', 'to', 'is', 'was', 'for', 'it', 'on', 'he', 'as', 'by', 'with', 'that', 'are', 'from', 'at', 'an', 'his', 'or', 'they', 'other', 'this', 'be', 'also', 'were', 'not', 'has']\n",
    "print(stop_words)\n",
    "vocab_inds = sorted([x[0] for x in word_counts.most_common(100029) if not x[0] in stop_inds])\n",
    "vocab = [word_list[x] for x in vocab_inds]\n",
    "word_freqs_short = [word_counts[x] for x in vocab_inds]\n",
    "print(len(vocab))\n",
    "#[6, 8, 17]\n",
    "print(vocab_inds[:3])\n",
    "word_to_num_short = {}\n",
    "word_list_short = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word_to_num_short[word] = i\n",
    "    word_list_short[i] = word\n",
    "    \n",
    "X_short = X[vocab_inds, :]\n",
    "X_short = X_short[:, vocab_inds]\n",
    "#(100000, 100000)\n",
    "print(X_short.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
